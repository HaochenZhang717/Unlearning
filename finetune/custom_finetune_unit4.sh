#!/bin/bash

# --- ç¡¬ä»¶é…ç½® ---
GPUS_PER_NODE=1
export CUDA_VISIBLE_DEVICES=2
export HF_HOME=/playpen/haochenz/hf_cache
MASTER_PORT=29501 # å»ºè®®æ¢ä¸€ä¸ªç«¯å£é˜²æ­¢å†²çª

# --- å›ºå®šè·¯å¾„é…ç½® ---
MODEL_ID="llava-hf/llava-1.5-7b-hf"
DATA_DIR="/playpen/haochenz/UMU-Bench/full_data/train-00000-of-00001.parquet"
BASE_SAVE_DIR="../checkpoints/llava-sweep"

# --- è®­ç»ƒå‚æ•° ---
BATCH_SIZE=3
GRAD_ACCUM=2
EPOCHS=5 # è®ºæ–‡å»ºè®®æ˜¯ 5

# --- ğŸ’¡ å®šä¹‰è¦æµ‹è¯•çš„å‚æ•°åˆ—è¡¨ ---
LR_LIST=(1e-4)
LORA_R_LIST=(8 16 32 64)

# --- å¼€å§‹åµŒå¥—å¾ªç¯ Sweep ---
for LR in "${LR_LIST[@]}"
do
  for R in "${LORA_R_LIST[@]}"
  do
    # åŠ¨æ€ç”Ÿæˆä¿å­˜è·¯å¾„å’Œä»»åŠ¡åç§°ï¼ŒåŒºåˆ† LR å’Œ R
    RUN_NAME="llava-lr-${LR}-r-${R}"
    CURRENT_SAVE_DIR="$BASE_SAVE_DIR/$RUN_NAME"

    mkdir -p $CURRENT_SAVE_DIR

    echo "------------------------------------------------"
    echo "ğŸš€ Starting Sweep: $RUN_NAME | LR=$LR, LoRA_R=$R"
    echo "------------------------------------------------"

    export WANDB_NAME=$RUN_NAME

    torchrun \
        --nproc_per_node=$GPUS_PER_NODE \
        --master_port=$MASTER_PORT \
        custom_finetune.py \
        --model_id $MODEL_ID \
        --data_dir $DATA_DIR \
        --save_dir $CURRENT_SAVE_DIR \
        --batch_size $BATCH_SIZE \
        --gradient_accumulation_steps $GRAD_ACCUM \
        --num_epochs $EPOCHS \
        --lr $LR \
        --lora_r $R \
        --lora_alpha 16

    echo "âœ… Finished Sweep for LR=$LR, R=$R"
  done
done

echo "All sweeps completed."