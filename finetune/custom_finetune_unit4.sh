#!/bin/bash

# --- ç¡¬ä»¶é…ç½® ---
GPUS_PER_NODE=1
export CUDA_VISIBLE_DEVICES=2

export HF_HOME=/playpen/haochenz/hf_cache
MASTER_PORT=29500

# --- å›ºå®šè·¯å¾„é…ç½® ---
MODEL_ID="llava-hf/llava-1.5-7b-hf"
DATA_DIR="/playpen/haochenz/UMU-Bench/full_data/train-00000-of-00001.parquet"
BASE_SAVE_DIR="../checkpoints/llava-sweep"

# --- å›ºå®šè®­ç»ƒå‚æ•° ---
BATCH_SIZE=3
GRAD_ACCUM=2
EPOCHS=3  # Sweep æ—¶å¯ä»¥é€‚å½“å‡å°‘ Epoch ä»¥èŠ‚çœæ—¶é—´

# --- ğŸ’¡ å®šä¹‰è¦æµ‹è¯•çš„å­¦ä¹ ç‡åˆ—è¡¨ ---
LR_LIST=(1e-5 2e-5 5e-5 1e-4)

# --- å¼€å§‹å¾ªç¯ Sweep ---
for LR in "${LR_LIST[@]}"
do
    # åŠ¨æ€ç”Ÿæˆä¿å­˜è·¯å¾„å’Œä»»åŠ¡åç§°ï¼Œä¾‹å¦‚ï¼šllava-lr-1e-5
    RUN_NAME="llava-lr-$LR"
    CURRENT_SAVE_DIR="$BASE_SAVE_DIR/$RUN_NAME"

    mkdir -p $CURRENT_SAVE_DIR

    echo "------------------------------------------------"
    echo "ğŸš€ Starting Sweep: $RUN_NAME with LR=$LR"
    echo "------------------------------------------------"

    # ä½¿ç”¨ torchrun å¯åŠ¨
    # æ³¨æ„ï¼šæˆ‘åœ¨åé¢æ·»åŠ äº† --run_name å‚æ•°ï¼ˆå‡è®¾ä½ åœ¨ python ä»£ç é‡Œå¤„ç†å®ƒï¼‰
    # æˆ–è€…é€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’ç»™ wandb
    export WANDB_NAME=$RUN_NAME

    torchrun \
        --nproc_per_node=$GPUS_PER_NODE \
        --master_port=$MASTER_PORT \
        custom_finetune.py \
        --model_id $MODEL_ID \
        --data_dir $DATA_DIR \
        --save_dir $CURRENT_SAVE_DIR \
        --batch_size $BATCH_SIZE \
        --gradient_accumulation_steps $GRAD_ACCUM \
        --num_epochs $EPOCHS \
        --lr $LR

    echo "âœ… Finished Sweep for LR=$LR"
done

echo "All sweeps completed."